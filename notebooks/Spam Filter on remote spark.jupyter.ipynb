{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Uploading the dataset to remote cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload success\n"
     ]
    }
   ],
   "source": [
    "# Push the dataset to user HDFS directory in the cluster\n",
    "import os\n",
    "import dsx_core_utils\n",
    "dsx_core_utils.upload_hdfs_file(\n",
    "    source_path=os.environ['DSX_PROJECT_DIR']+'/datasets/SMSSpamCollection.csv', \n",
    "    target_path=\"/user/user1/SMSSpamCollection.csv\",\n",
    "    webhdfsurl=\"https://huey1.fyre.ibm.com:8443/gateway/yc375-master-1/webhdfs/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### List the available environemnts in Hadoop Integration Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Hadoop systems: \n",
      "\n",
      "['huey1', 'bendy1', 'yccdh5']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'LIVYSPARK': 'https://huey1.fyre.ibm.com:8443/gateway/yc375-master-1/livy/v1',\n",
       "  'LIVYSPARK2': 'https://huey1.fyre.ibm.com:8443/gateway/yc375-master-1/livy2/v1',\n",
       "  'WEBHDFS': 'https://huey1.fyre.ibm.com:8443/gateway/yc375-master-1/webhdfs/v1',\n",
       "  'imageIdNameList': [{'id': 'dsx-scripted-ml-python3',\n",
       "    'name': 'Jupyter with Python 3.5, Scala 2.11, R 3.4.3'},\n",
       "   {'id': 'dsx-scripted-ml-python2',\n",
       "    'name': 'Jupyter with Python 2.7, Scala 2.11, R 3.4.3'}],\n",
       "  'imageList': ['dsx-scripted-ml-python3', 'dsx-scripted-ml-python2'],\n",
       "  'serviceUserID': 'dsxhi',\n",
       "  'system': 'huey1'},\n",
       " {'LIVYSPARK': 'https://bendy1.fyre.ibm.com:8443/gateway/yc375-master-1/livy/v1',\n",
       "  'LIVYSPARK2': 'https://bendy1.fyre.ibm.com:8443/gateway/yc375-master-1/livy2/v1',\n",
       "  'WEBHDFS': 'https://bendy1.fyre.ibm.com:8443/gateway/yc375-master-1/webhdfs/v1',\n",
       "  'imageIdNameList': [{'id': 'dsx-scripted-ml-python2',\n",
       "    'name': 'Jupyter with Python 2.7, Scala 2.11, R 3.4.3'},\n",
       "   {'id': 'dsx-scripted-ml-python3',\n",
       "    'name': 'Jupyter with Python 3.5, Scala 2.11, R 3.4.3'}],\n",
       "  'imageList': ['dsx-scripted-ml-python2', 'dsx-scripted-ml-python3'],\n",
       "  'serviceUserID': 'dsxhi',\n",
       "  'system': 'bendy1'},\n",
       " {'LIVYSPARK': 'https://yccdh5.fyre.ibm.com:8443/gateway/yc375-master-1/livy/v1',\n",
       "  'LIVYSPARK2': 'https://yccdh5.fyre.ibm.com:8443/gateway/yc375-master-1/livy2/v1',\n",
       "  'WEBHDFS': 'https://yccdh5.fyre.ibm.com:8443/gateway/yc375-master-1/webhdfs/v1',\n",
       "  'imageIdNameList': [{'id': 'dsx-scripted-ml-python3',\n",
       "    'name': 'Jupyter with Python 3.5, Scala 2.11, R 3.4.3'},\n",
       "   {'id': 'dsx-scripted-ml-python2',\n",
       "    'name': 'Jupyter with Python 2.7, Scala 2.11, R 3.4.3'}],\n",
       "  'imageList': ['dsx-scripted-ml-python3', 'dsx-scripted-ml-python2'],\n",
       "  'serviceUserID': 'dsxhi',\n",
       "  'system': 'yccdh5'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dsx_core_utils;\n",
    "dsx_core_utils.get_dsxhi_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Connecting to remote spark using Hadoop Integration Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparkmagic has been configured to use https://huey1.fyre.ibm.com:8443/gateway/yc375-master-1/livy2/v1 with image Jupyter with Python 2.7, Scala 2.11, R 3.4.3\n",
      "success configuring sparkmagic livy.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'IMAGE': {'ID': 'dsx-scripted-ml-python2',\n",
       "  'NAME': 'Jupyter with Python 2.7, Scala 2.11, R 3.4.3'},\n",
       " 'LIVY': 'https://huey1.fyre.ibm.com:8443/gateway/yc375-master-1/livy2/v1',\n",
       " 'SYSTEM': 'huey1',\n",
       " 'WEBHDFS': 'https://huey1.fyre.ibm.com:8443/gateway/yc375-master-1/webhdfs/v1'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dsx_core_utils\n",
    "\n",
    "dsxhiRegisterDisplayName=\"huey1\"\n",
    "imageID= \"dsx-scripted-ml-python2\"\n",
    "livy_name = \"livyspark2\"\n",
    "dsx_core_utils.setup_livy_sparkmagic(\n",
    "    system=dsxhiRegisterDisplayName, \n",
    "    livy=livy_name, \n",
    "    imageId=imageID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%reload_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>44</td><td>application_1535429375180_0026</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://huey2.fyre.ibm.com:8088/proxy/application_1535429375180_0026/\">Link</a></td><td><a target=\"_blank\" href=\"http://huey1.fyre.ibm.com:8042/node/containerlogs/container_e04_1535429375180_0026_01_000001/user1\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%spark add -s s4 -l python -u https://huey1.fyre.ibm.com:8443/gateway/yc375-master-1/livy2/v1 -k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reading the dataset from remote cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs:///user/user1/SMSSpamCollection.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "smsData = sc.textFile(\"hdfs:///user/user1/SMSSpamCollection.csv\")\n",
    "smsData.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Creating a data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF    \n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "    \n",
    "tokenizer = Tokenizer(inputCol=\"message\",outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol = tokenizer.getOutputCol(),outputCol=\"tempfeatures\")\n",
    "idf = IDF(inputCol = hashingTF.getOutputCol(),outputCol=\"features\")\n",
    "lrClassifier = LogisticRegression()\n",
    "        \n",
    "pipeline = Pipeline(stages=[tokenizer,hashingTF,idf,lrClassifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[label: double, message: string]"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# creating a labeled vector\n",
    "def TransformToVector(string):\n",
    "    attList = string.split(\",\")\n",
    "    smsType = 0.0 if attList[0] == \"ham\" else 1.0\n",
    "    return [smsType,attList[1]]\n",
    "        \n",
    "smsTransformed = smsData.map(TransformToVector)\n",
    "        \n",
    "# creating a data frame from labeled vector\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "smsDF = sqlContext.createDataFrame(smsTransformed,[\"label\",\"message\"])\n",
    "smsDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Perform Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 89.09\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|   50|\n",
      "|  0.0|       1.0|    3|\n",
      "|  1.0|       0.0|    9|\n",
      "|  0.0|       0.0|   48|\n",
      "+-----+----------+-----+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "# split data frame into training and testing\n",
    "(trainingData,testData) = smsDF.randomSplit([0.9,0.1])\n",
    "        \n",
    "#Build a model with Pipeline\n",
    "lrModel = pipeline.fit(trainingData)\n",
    "        \n",
    "#Compute Predictions\n",
    "prediction = lrModel.transform(testData)\n",
    "            \n",
    "#Evaluate Accuracy\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                                                      labelCol=\"label\", \\\n",
    "                                                      metricName = \"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print \"Model Accuracy: \" + str(round(accuracy*100,2))\n",
    "        \n",
    "# Draw a confusion matrix\n",
    "prediction.groupby(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='model_HDFS_dump'></a>\n",
    "## Save and Dump the Model to remote HDFS\n",
    "The model now exists within the memory of the remote livy session. In order to use it in Watson Studio model management, we need to transfer it to the local Watson Studio environment.  This is done in two parts.\n",
    "\n",
    "First, in the _remote_ session:\n",
    "\n",
    "  * Write the model to HDFS as a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%spark \n",
    "#Save model to HDFS\n",
    "model_name = \"LRModel_SparkRemote\"\n",
    "hdfs_model_path = \"/tmp/\"+model_name\n",
    "lrModel.write().overwrite().save(hdfs_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "  * Pull the model directory from HDFS to the driver node's local fs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x 4 yarn hadoop 36 Oct 25 13:00 ./tmp/LRModel_SparkRemote"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "import time, subprocess\n",
    "local_model_path = \"./tmp/\"+model_name\n",
    "output1 = subprocess.Popen((\"hdfs dfs -copyToLocal \" + hdfs_model_path + \" \" + local_model_path), shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True)\n",
    "time.sleep(10)\n",
    "output2 = subprocess.Popen((\"ls -ld ./tmp/LRModel_SparkRemote\"), shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True)\n",
    "print output1.communicate()[0], output2.communicate()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "  * Create an archive of the model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/hadoop/yarn/local/usercache/user1/appcache/application_1535429375180_0026/container_e04_1535429375180_0026_01_000001/tmp/LRModel_SparkRemote.tar.gz'"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "import shutil, os\n",
    "os.chdir(\"./tmp\")\n",
    "shutil.make_archive( base_name =model_name, format= 'gztar', root_dir = model_name +'/',  base_dir  =  './' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "  * Push the archive back up to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -rw-r--r--   3 yarn hdfs      42353 2018-10-25 13:02 /tmp/LRModel_SparkRemote.tar.gz"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "hdfs_model_dir = \"/tmp\"\n",
    "output3 = subprocess.Popen((\"hdfs dfs -copyFromLocal -f LRModel_SparkRemote.tar.gz \" + hdfs_model_dir), shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True)\n",
    "time.sleep(10)\n",
    "output4 = subprocess.Popen((\"hdfs dfs -ls /tmp/LRModel_SparkRemote.tar.gz\"), shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, close_fds=True)\n",
    "print output3.communicate()[0], output4.communicate()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='model_hdfs_load'></a>\n",
    "## Load the model from HDFS into Watson Studio\n",
    "Then, on the Watson Studio _local_ side:\n",
    "  * Download the model archive from HDFS to a temporary directory in Watson Studio's filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download success\n"
     ]
    }
   ],
   "source": [
    "from dsx_core_utils import hdfs_util\n",
    "\n",
    "localmpdir = os.environ[\"DSX_PROJECT_DIR\"] + \"/.tmp/LRModel_SparkRemote/\"\n",
    "!mkdir -p $localmpdir\n",
    "\n",
    "# Download the model archive from HDFS.\n",
    "hdfs_models_dir = \"/tmp\"\n",
    "webhdfs_endpoint = \"https://huey1.fyre.ibm.com:8443/gateway/yc375-master-1/webhdfs/v1\"\n",
    "hdfs_util.download_file(webhdfs_endpoint,hdfs_models_dir+\"/LRModel_SparkRemote.tar.gz\",localmpdir+\"LRModel_SparkRemote.tar.gz\")\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 50\r\n",
      "-rw-rw-rw-. 1 1001 allDSXUsers 42301 Oct 25 20:03 LRModel_SparkRemote.tar.gz\r\n",
      "drwxr-xr-x. 2 1001 allDSXUsers  4096 Oct 25 17:52 metadata\r\n",
      "drwxr-xr-x. 6 1001 allDSXUsers  4096 Oct 25 17:52 stages\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l $localmpdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "  * Unpack the model archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!tar -xzf $localmpdir/LRModel_SparkRemote.tar.gz -C $localmpdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 50\r\n",
      "-rw-rw-rw-.  1 1001 allDSXUsers 42301 Oct 25 20:03 LRModel_SparkRemote.tar.gz\r\n",
      "drwxr-xr-x.  2 1001 allDSXUsers  4096 Oct 25 20:00 metadata\r\n",
      "drwxr-xr-x. 10 1001 allDSXUsers  4096 Oct 25 20:00 stages\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l $localmpdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "  * Load the model into memory from the unpacked archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "lrModel = PipelineModel.load(localmpdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='save_model'></a>\n",
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/user-home/1001/DSX_Projects/sms-spam-filter-using-hortonworks/models/LRModel_SparkRemote/1',\n",
       " 'scoring_endpoint': 'https://dsxl-api/v3/project/score/Python35/spark-2.2/sms-spam-filter-using-hortonworks/LRModel_SparkRemote/1'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dsx_ml.ml import save\n",
    "save(name = 'LRModel_SparkRemote',\n",
    "     model = lrModel,\n",
    "     test_data = testData,\n",
    "     algorithm_type = 'Classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "testData = SQLContext(sc).read.csv(os.environ['DSX_PROJECT_DIR']+'/datasets/sms-data.csv', header='true', inferSchema = 'true')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
